# This is the executor pod template for the XGBoost example.
# The executor pod is launched in the same node group as the Spark executor.
# Make sure to update the availability zone and node group name to match your cluster.
# The toleration is required to allow the executor pod to be scheduled on the same node as the Spark executor.
# /local1 is the local disk mount point on the node. This is used for shuffle data.
apiVersion: v1
kind: Pod
metadata:
  name: nvidia-demo-executor
  namespace: emr-ml-team-a

spec:
  nodeSelector:
    # NodeGroupType: "spark-executor-gpu-ca" # Comment this line if you want to use Karpenter autosclaing
    NodeGroupType: spark-executor-gpu-karpenter # Uncomment this line if you want to use Karpenter autosclaing
  containers:
  - name: spark-kubernetes-executor # Don't change this name. EMR on EKS looking for this name
    volumeMounts:
    - name: spark-local-dir-1
      mountPath: /data1
  initContainers:
  - name: volume-permission
    image: public.ecr.aws/docker/library/busybox
    command: [ 'sh', '-c', 'mkdir -p /data1; chown -R 185:185 /data1' ]
    volumeMounts:
    - name: spark-local-dir-1
      mountPath: /data1
  volumes:
  - name: spark-local-dir-1
    hostPath:
      path: "/mnt/k8s-disks/0"
      type: DirectoryOrCreate
  tolerations:
  - key: "nvidia.com/gpu"
    operator: "Equal"
    value: "present"
    effect: "NoSchedule"
